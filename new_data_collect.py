#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
High-Performance Game Collector  v3
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Â· ë©€í‹° API í‚¤ Â· 4-120ë¶„ Â· í•œê¸€ í¬í•¨
Â· ìë§‰ txt ì €ì¥ (ìˆ˜ë™/ìë™/ë²ˆì—­ ê°ì§€)
Â· ì¤‘ë³µ ID í•„í„°, quota 9 500 unit ì œì–´, ì¬ì‹œë„Â·ë°±ì˜¤í”„
"""

from googleapiclient.discovery import build
from googleapiclient.errors import HttpError

# â”€â”€â”€ youtube_transcript_api (ë²„ì „ í˜¸í™˜) â”€â”€â”€
from youtube_transcript_api import (
    YouTubeTranscriptApi,
    TranscriptsDisabled,
    NoTranscriptFound,
    CouldNotRetrieveTranscript,
)

# TooManyRequests: 0.6.x = ìµœìƒìœ„, 1.x = _errors ë‚´ë¶€, ë” ì˜›ë‚  = ì—†ìŒ
try:
    from youtube_transcript_api import TooManyRequests              # 0.6.x
except ImportError:
    try:
        from youtube_transcript_api._errors import TooManyRequests   # 1.x
    except ImportError:
        class TooManyRequests(Exception):                           # fallback
            """Placeholder for missing TooManyRequests exception"""
            pass

from dateutil import parser as dtp
from datetime import datetime
from collections import defaultdict
import isodate, pandas as pd, os, re, sys, argparse, time, random, json

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì „ì—­ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
API_KEYS = {
    "ë¡¤":         "AIzaSyDPvRKZPzQXnYdOonKEJ8y7J0Ufc5zMuXA",
    "ì„œë“ ":       "AIzaSyDHk9Fd_6plv_wP3Oie83P-4Wnir3h4eg4",
    "FCì˜¨ë¼ì¸":   "AIzaSyCB097p34VBzavqHkripR-AQRfZOYkAO2Y",
    "ë°°í‹€ê·¸ë¼ìš´ë“œ":"AIzaSyDRE48qtjqYwxNnypL8nZn62qqcuIW7BZQ",
    "ë°œë¡œë€íŠ¸":   "AIzaSyB8xrapCqROGfnt8hBkBmSncG1rtZu2Wfw"
}

CSV_NAME               = "game_api_data.csv"
TXT_DIR                = "transcript_api"     # â¬…ï¸ NEW
MIN_SEC, MAX_SEC       = 240, 7200
MAX_CH_SEARCH_PAGES    = 2
MAX_PL_PAGES           = 11
UNIT_LIMIT_PER_DAY     = 9_500

COST = {"search": 100, "channel": 1, "plist": 1, "videos": 1}

TAG_STR = {
    "ë¡¤":         "ë¦¬ê·¸ì˜¤ë¸Œë ˆì „ë“œ|ë¡¤|league of legends|LOL",
    "ì„œë“ ":       "ì„œë“ ì–´íƒ|ì„œë“ ",
    "FCì˜¨ë¼ì¸":   "FCì˜¨ë¼ì¸|í”¼íŒŒì˜¨ë¼ì¸|í”¼íŒŒ|EA FC Online",
    "ë°°í‹€ê·¸ë¼ìš´ë“œ":"ë°°í‹€ê·¸ë¼ìš´ë“œ|ë°°ê·¸|PUBG",
    "ë°œë¡œë€íŠ¸":   "ë°œë¡œë€íŠ¸|ë°œë¡œ|VALORANT",
}

kor       = re.compile(r"[ê°€-í£]")
safe_int  = lambda x: int(x) if str(x).isdigit() else 0
os.makedirs(TXT_DIR, exist_ok=True)            # â¬…ï¸ NEW

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê³µí†µ í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
def log(*m): print("[", datetime.now().strftime("%H:%M:%S"), "]", *m, flush=True)

def hms(sec:int)->str:
    h,m = divmod(sec,3600); m,s = divmod(m,60)
    return f"{h}:{m:02d}:{s:02d}" if h else f"{m:02d}:{s:02d}"

def len_ok(iso):
    s = int(isodate.parse_duration(iso).total_seconds())
    return MIN_SEC <= s <= MAX_SEC, s

# â”€â”€â”€â”€â”€â”€â”€â”€ ìë§‰ ì €ì¥ (FIX + NEW) â”€â”€â”€â”€â”€â”€â”€â”€ #
def transcript_save(video_id: str) -> str:
    """
    â€¢ ê°€ëŠ¥í•œ ê²½ìš° í•œê¸€(ko) â†’ ì˜ì–´(en) â†’ ìµœì´ˆ íŠ¸ëœìŠ¤í¬ë¦½íŠ¸ ìˆœìœ¼ë¡œ ì„ íƒ
    â€¢ txt ì €ì¥:  [TXT_DIR]/<video_id>.txt
    â€¢ ë°˜í™˜ê°’: 'manual-ko' / 'auto-ko' / 'translate-ko' / 'none' ë“±
    """
    try:
        list_obj = YouTubeTranscriptApi.list_transcripts(video_id)

        # ìš°ì„ ìˆœìœ„ 1) ìˆ˜ë™ ko, 2) ìë™ ko, 3) ìˆ˜ë™ en, 4) ìë™ en, 5) ì²« íŠ¸ëœìŠ¤í¬ë¦½íŠ¸
        target = (
            list_obj.find_manually_created_transcript(['ko', 'ko-KR']) or
            list_obj.find_generated_transcript(['ko', 'ko-KR'])         or
            list_obj.find_manually_created_transcript(['en'])           or
            list_obj.find_generated_transcript(['en'])
        )
        if target is None:
            target = list_obj._TranscriptsList__transcripts[0]  # ì²« ë²ˆì§¸ fallback

        # ì‹¤ì œ ë°›ê¸° (í•œêµ­ì–´ ì—†ìœ¼ë©´ ë²ˆì—­)
        if 'ko' not in target.language_code:
            try:
                target = target.translate('ko')
                t_type = f"translate-{target.language_code}"
            except Exception:
                t_type = f"{'manual' if target.is_manually_created else 'auto'}-{target.language_code}"
        else:
            t_type = f"{'manual' if target.is_manually_created else 'auto'}-ko"

        lines = target.fetch()
        # ì‹œê°„+í…ìŠ¤íŠ¸ â†’ í…ìŠ¤íŠ¸ë§Œ
        transcript_text = "\n".join([l['text'] for l in lines]).strip()
        if transcript_text:
            with open(os.path.join(TXT_DIR, f"{video_id}.txt"), "w", encoding="utf-8") as f:
                f.write(transcript_text)
        else:
            t_type = "none"

        return t_type
    except (TranscriptsDisabled, NoTranscriptFound):
        return "disabled"
    except TooManyRequests:
        log("âš ï¸ Transcript TooManyRequests â€“ sleep 30s")
        time.sleep(30)
        return transcript_save(video_id)  # ì¬ê·€ ì¬ì‹œë„(ë‹¨ìˆœ)
    except CouldNotRetrieveTranscript:
        return "error"
    except Exception as e:
        log("âš ï¸ Transcript error:", e)
        return "error"

# â”€â”€â”€â”€â”€â”€â”€â”€ API ì‚¬ìš©ëŸ‰ ê´€ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€ #
API_USAGE = defaultdict(int)

def charge(key:str, units:int):
    if API_USAGE[key] + units > UNIT_LIMIT_PER_DAY:
        raise StopIteration(f"quota {API_USAGE[key]}â†’{API_USAGE[key]+units} unit ì´ˆê³¼")
    API_USAGE[key] += units

def exec_request(req, cost, api_key, retries=3):
    delay = 2
    for n in range(retries):
        try:
            charge(api_key, cost)
            return req.execute()
        except HttpError as e:
            if e.resp.status in (500, 503, 429) or (e.resp.status==403 and "quotaExceeded" in str(e)):
                log(f"âš ï¸ {e.resp.status} ì¬ì‹œë„ {n+1}/{retries}")
                time.sleep(delay + random.random())
                delay *= 2
                continue
            raise
    raise RuntimeError("API ì¬ì‹œë„ ì‹¤íŒ¨")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Data API ë˜í¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
def s_list(y, k, **kw): return exec_request(y.search().list(**kw), COST["search"], k)
def ch_list(y, k, ids):   return exec_request(y.channels().list(
                            part="snippet,statistics,contentDetails,brandingSettings",
                            id=",".join(ids)), COST["channel"]*len(ids), k)
def pl_list(y, k, pid, tok=None): return exec_request(
                            y.playlistItems().list(part="contentDetails", playlistId=pid,
                                                   maxResults=50, pageToken=tok),
                            COST["plist"], k)
def v_list(y, k, ids): return exec_request(
                            y.videos().list(part="snippet,contentDetails,statistics,status",
                                            id=",".join(ids)),
                            COST["videos"]*len(ids), k)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë©”ì¸ ìˆ˜ì§‘ ë£¨í‹´ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
def collect_game(game, query, api_key, start_date, end_date, exist_ids):
    yt  = build("youtube", "v3", developerKey=api_key)
    today = datetime.now().strftime("%Y-%m-%d")
    START = dtp.parse(start_date).date()
    END   = dtp.parse(end_date).date()
    terms = query.split("|")

    records = []
    try:
        uploads, ch_meta, nxt, pg = set(), {}, None, 0
        while pg < MAX_CH_SEARCH_PAGES:
            res = s_list(yt, api_key, part="id", q=query, type="channel",
                         maxResults=50, pageToken=nxt)
            ids = [c['id']['channelId'] for c in res.get("items", [])]
            if ids:
                meta = ch_list(yt, api_key, ids)
                for ch in meta.get("items", []):
                    kw = (ch["brandingSettings"]["channel"].get("keywords") or "").lower()
                    if any(t in kw for t in terms):
                        pid = ch["contentDetails"]["relatedPlaylists"]["uploads"]
                        uploads.add(pid)
                        ch_meta[pid] = ch
            nxt, pg = res.get("nextPageToken"), pg+1
            if not nxt: break

        for up in uploads:
            vids, nxt, pp = [], None, 0
            while pp < MAX_PL_PAGES:
                pl = pl_list(yt, api_key, up, nxt)
                for it in pl.get("items", []):
                    vid = it["contentDetails"]["videoId"]
                    if vid in exist_ids: continue
                    pub = dtp.isoparse(it["contentDetails"]["videoPublishedAt"]).replace(tzinfo=None).date()
                    if START <= pub <= END:
                        vids.append(vid)
                nxt, pp = pl.get("nextPageToken"), pp+1
                if not nxt: break

            for i in range(0, len(vids), 50):
                metas = v_list(yt, api_key, vids[i:i+50]).get("items", [])
                for v in metas:
                    sn, cd, st = v["snippet"], v["contentDetails"], v.get("statistics", {})
                    ok, sec = len_ok(cd["duration"]);  title = sn.get("title") or ""
                    if not ok: continue
                    txt = (title + (sn.get("description") or "")).lower()
                    if not kor.search(txt) or not any(t in txt for t in terms): continue

                    # â¬…ï¸ NEW: ìë§‰ ì €ì¥
                    caption_type = transcript_save(v["id"])

                    ch = ch_meta.get(up, {})
                    records.append({
                        "ìˆ˜ì§‘ì¼ì": today,
                        "ê²Œì„ëª…": game,
                        "ì˜ìƒID": v["id"],
                        "ì˜ìƒì œëª©": title,
                        "ì˜ìƒì„¤ëª…": sn.get("description") or "",
                        "ê²Œì‹œì¼": sn.get("publishedAt"),
                        "ì±„ë„ID": sn.get("channelId"),
                        "ì±„ë„ëª…": sn.get("channelTitle"),
                        "íƒœê·¸": ", ".join(sn.get("tags") or []),
                        "ì¹´í…Œê³ ë¦¬ID": sn.get("categoryId"),
                        "ì¸ë„¤ì¼URL": sn.get("thumbnails", {}).get("high", {}).get("url"),
                        "defaultLanguage": sn.get("defaultLanguage"),
                        "duration": cd.get("duration"),
                        "ì˜ìƒê¸¸ì´(ì´ˆ)": sec,
                        "ì˜ìƒê¸¸ì´(í‘œì‹œ)": hms(sec),
                        "dimension": cd.get("dimension"),
                        "definition": cd.get("definition"),
                        "caption": cd.get("caption"),
                        "licensedContent": cd.get("licensedContent"),
                        "privacyStatus": v["status"].get("privacyStatus"),
                        "madeForKids": v["status"].get("madeForKids"),
                        "viewCount": safe_int(st.get("viewCount")),
                        "likeCount": safe_int(st.get("likeCount")),
                        "commentCount": safe_int(st.get("commentCount")),
                        "êµ¬ë…ììˆ˜": safe_int(ch.get("statistics", {}).get("subscriberCount")),
                        "ì±„ë„ì´ì¡°íšŒìˆ˜": safe_int(ch.get("statistics", {}).get("viewCount")),
                        "ì±„ë„ì—…ë¡œë“œì˜ìƒìˆ˜": safe_int(ch.get("statistics", {}).get("videoCount")),
                        "ì±„ë„ê°œì„¤ì¼": ch.get("snippet", {}).get("publishedAt"),
                        "ìë§‰ìœ í˜•": caption_type                      # â¬…ï¸ NEW
                    })
    except StopIteration as e:
        log(f"â–¶ {game} ì¤‘ë‹¨ â€“ {e}")

    log(f"== {game} ì™„ë£Œ : {len(records)}í¸ ìˆ˜ì§‘")
    return records

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì§„ì…ì  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--start", required=True, help="2024-01-01")
    parser.add_argument("--end",   required=True, help="2024-01-31")
    args = parser.parse_args()

    try:
        s_date = dtp.parse(args.start).date()
        e_date = dtp.parse(args.end).date()
        if s_date > e_date:
            parser.error("--start ëŠ” --end ì´ì „ì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
    except Exception:
        parser.error("ë‚ ì§œ í˜•ì‹ì€ YYYY-MM-DD ì´ì–´ì•¼ í•©ë‹ˆë‹¤.")

    # ê¸°ì¡´ CSV ì¤‘ë³µ í•„í„°
    exist_ids = set()
    if os.path.exists(CSV_NAME):
        for chunk in pd.read_csv(CSV_NAME, usecols=["ì˜ìƒID"], dtype=str, chunksize=100_000):
            exist_ids.update(chunk["ì˜ìƒID"].tolist())
        log(f"ì¤‘ë³µ í•„í„°: {len(exist_ids):,}ê°œ ì˜ìƒID ë¡œë“œ")

    all_rec = []
    for game, query in TAG_STR.items():
        key = API_KEYS.get(game)
        if not key:
            log(f"API í‚¤ ëˆ„ë½ â†’ {game} ê±´ë„ˆëœ€"); continue
        recs = collect_game(game, query, key, args.start, args.end, exist_ids)
        all_rec.extend(recs); exist_ids.update(r["ì˜ìƒID"] for r in recs)

    if all_rec:
        pd.DataFrame(all_rec).to_csv(
            CSV_NAME, mode="a",
            header=not os.path.exists(CSV_NAME), index=False,
            encoding="utf-8-sig")
        log(f"ğŸ’¾ {len(all_rec):,} ê±´ ì €ì¥ ì™„ë£Œ")
    log("ğŸŒ™ ì „ì²´ ì‹¤í–‰ ì¢…ë£Œ")

if __name__ == "__main__":
    main()
