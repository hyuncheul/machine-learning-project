from googleapiclient.discovery import build
from youtube_transcript_api import YouTubeTranscriptApi
import isodate
import pandas as pd
import os
import time
from datetime import datetime, timedelta


API_KEY = 'AIzaSyBVfH5AA4ht6Y_mJ-JtiuV3eSK6l_5pJyU'  
search_queries = ['ë¡¤', 'ë°°í‹€ê·¸ë¼ìš´ë“œ', 'ë‹Œí…ë„', 'ëª¨ë°”ì¼ ê²Œì„', 'ê²Œì„ ë¦¬ë·°', 'ì¸ë”” ê²Œì„']
max_total = 500
per_request = 50

captions_folder = 'captions'
os.makedirs(captions_folder, exist_ok=True)

today = datetime.utcnow()
today_str = today.strftime('%Y%m%d')
csv_output_path = f'youtube_videos_{today_str}.csv'
today_ids_path = f'collected_ids_{today_str}.txt'
total_ids_path = 'collected_ids_total.txt'

# ê²Œì‹œì¼ ì¡°ê±´: 60ì¼ ì „ ~ 5ì¼ ì „
published_before = (today - timedelta(days=5)).isoformat("T") + "Z"
published_after = (today - timedelta(days=60)).isoformat("T") + "Z"

# ================== ê¸°ì¡´ ìˆ˜ì§‘ ì˜ìƒ ë¶ˆëŸ¬ì˜¤ê¸° ==================
all_collected_ids = set()
if os.path.exists(total_ids_path):
    with open(total_ids_path, 'r', encoding='utf-8') as f:
        all_collected_ids = set(line.strip() for line in f)

# ë‹¤ì‹œ ìˆ˜ì§‘í•  ê²½ìš° ì½”ë“œ ì‚¬ìš©
today_collected_ids = set()
# ë‹¤ì‹œ ìˆ˜ì§‘í•  ê²½ìš° ì½”ë“œ ì‚¬ìš©
'''if os.path.exists(today_ids_path):
    os.remove(today_ids_path)  # âœ… ì˜¤ëŠ˜ì ID ì´ˆê¸°í™”

# ê¸°ì¡´ CSV ë¶ˆëŸ¬ì˜¤ê¸° (ì˜¤ëŠ˜ì CSVëŠ” ì‚­ì œ)
if os.path.exists(csv_output_path):
    os.remove(csv_output_path)
'''
video_data = []

# =================== API ê°ì²´ ìƒì„± ====================
youtube = build('youtube', 'v3', developerKey=API_KEY)

# ===================== ìˆ˜ì§‘ ì‹œì‘ ======================
for query in search_queries:
    print(f"\nğŸ” ê²€ìƒ‰ì–´: {query}")
    collected = 0
    next_page_token = None

    while collected < max_total:
        response = youtube.search().list(
            part='snippet',
            type='video',
            maxResults=min(per_request, max_total - collected),
            regionCode='KR',
            videoCategoryId='20',
            q=query,
            videoDuration='long',
            order='viewCount',
            pageToken=next_page_token
        ).execute()

        items = response.get('items', [])
        video_ids = [item['id']['videoId'] for item in items]
        channel_ids = [item['snippet']['channelId'] for item in items]

        # ì¤‘ë³µ ì œê±° (ì˜¤ëŠ˜ì ì œì™¸í•˜ê³  ë¹„êµ)
        new_video_ids = [vid for vid in video_ids if vid not in (all_collected_ids - today_collected_ids)]
        if not new_video_ids:
            print("ìƒˆ ì˜ìƒ ì—†ìŒ, ì¢…ë£Œ")
            break

        video_response = youtube.videos().list(
            part='snippet,contentDetails,statistics',
            id=','.join(new_video_ids)
        ).execute()

        channel_response = youtube.channels().list(
            part='statistics',
            id=','.join(set(channel_ids))
        ).execute()
        channel_stats = {item['id']: item['statistics'] for item in channel_response['items']}

        for item in video_response['items']:
            snippet = item['snippet']
            stats = item.get('statistics', {})
            content = item.get('contentDetails', {})
            video_id = item['id']
            channel_id = snippet['channelId']
            channel_stat = channel_stats.get(channel_id, {})

            # ê²Œì‹œì¼ í•„í„°
            published_at = snippet.get('publishedAt', '')
            if not (published_after <= published_at <= published_before):
                continue

            # ìë§‰ ì €ì¥
            try:
                transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['ko', 'en'])
                transcript_text = '\n'.join([line['text'] for line in transcript])
                caption_path = os.path.join(captions_folder, f'{video_id}.txt')
                with open(caption_path, 'w', encoding='utf-8') as f:
                    f.write(transcript_text)
                caption_note = caption_path
                caption_type = 'transcript_api'
            except Exception:
                caption_note = ''
                caption_type = ''

            video_data.append({
                'ì˜ìƒID': video_id,
                'ì˜ìƒì œëª©': snippet.get('title', ''),
                'ì˜ìƒì„¤ëª…': snippet.get('description', ''),
                'ê²Œì‹œì¼': published_at,
                'íƒœê·¸': ', '.join(snippet.get('tags', [])) if 'tags' in snippet else '',
                'ì¹´í…Œê³ ë¦¬': snippet.get('categoryId', ''),
                'ì¸ë„¤ì¼': snippet.get('thumbnails', {}).get('high', {}).get('url', ''),
                'ì¡°íšŒìˆ˜': stats.get('viewCount', ''),
                'ì¢‹ì•„ìš”': stats.get('likeCount', ''),
                'ì˜ìƒê¸¸ì´(ì´ˆ)': isodate.parse_duration(content.get('duration', 'PT0S')).total_seconds(),
                'ìë§‰ì—¬ë¶€': content.get('caption', ''),
                'êµ¬ë…ì ìˆ˜': channel_stat.get('subscriberCount', ''),
                'ì±„ë„ ì´ ì¡°íšŒìˆ˜': channel_stat.get('viewCount', ''),
                'ì—…ë¡œë“œëœ ì˜ìƒ ìˆ˜': channel_stat.get('videoCount', ''),
                'ìë§‰ ìœ í˜•': caption_type,
                'ìë§‰ íŒŒì¼': caption_note
            })

            today_collected_ids.add(video_id)

        collected += len(new_video_ids)
        print(f"ëˆ„ì  ìˆ˜ì§‘: {collected}ê°œ")

        next_page_token = response.get('nextPageToken')
        if not next_page_token:
            break
        time.sleep(1)

# ===================== ê²°ê³¼ ì €ì¥ ======================
# CSV ì €ì¥
df = pd.DataFrame(video_data)
df.to_csv(csv_output_path, index=False, encoding='utf-8-sig')
print(f"âœ… CSV ì €ì¥ ì™„ë£Œ: {csv_output_path}")

# ì˜¤ëŠ˜ì ID ì €ì¥
with open(today_ids_path, 'w', encoding='utf-8') as f:
    for vid in today_collected_ids:
        f.write(vid + '\n')
print(f"âœ… ì˜¤ëŠ˜ì videoId ì €ì¥ ì™„ë£Œ: {today_ids_path}")

# ì „ì²´ ID ì—…ë°ì´íŠ¸ (ì˜¤ëŠ˜ì í¬í•¨)
all_collected_ids.update(today_collected_ids)
with open(total_ids_path, 'w', encoding='utf-8') as f:
    for vid in all_collected_ids:
        f.write(vid + '\n')
print(f"âœ… ëˆ„ì  videoId ì €ì¥ ì™„ë£Œ: {total_ids_path}")