from googleapiclient.discovery import build
from youtube_transcript_api import YouTubeTranscriptApi
from youtube_transcript_api.formatters import TextFormatter
import isodate
import pandas as pd
import os
import time
from datetime import datetime
import json
import random

# -------------------- ì„¤ì • ê°’ -------------------- #
API_KEY = 'AIzaSyCzTDt8mVszaZQoaRTIsjrrEf7M_dOEREE'
if not API_KEY:
    print("ì˜¤ë¥˜: API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    exit()

SEARCH_QUERY = 'ê²Œì„'  # ê²€ìƒ‰í•  í‚¤ì›Œë“œ
MAX_TOTAL_VIDEOS_TO_COLLECT = 100  # ìˆ˜ì§‘í•  ìµœëŒ€ ì˜ìƒ ê°œìˆ˜
VIDEOS_PER_REQUEST = 50  # í•œ ë²ˆì˜ API ìš”ì²­ìœ¼ë¡œ ê°€ì ¸ì˜¬ ìµœëŒ€ ì˜ìƒ ê°œìˆ˜ (50ë³´ë‹¤ ì‘ê²Œ ì„¤ì •í•˜ì—¬ í• ë‹¹ëŸ‰ ê´€ë¦¬)
PUBLISHED_AFTER_DATE = "2024-01-01T00:00:00Z"  # 2024ë…„ ì´í›„ ì˜ìƒ
MINIMUM_VIDEO_DURATION_SECONDS = 480  # ì˜ìƒ ê¸¸ì´ í•„í„°ë§ ê¸°ì¤€ (ì´ˆ ë‹¨ìœ„, 8ë¶„ = 480ì´ˆ)
MAX_VIDEO_DURATION_SECONDS = 4000  # ìµœëŒ€ ì˜ìƒ ê¸¸ì´ (2ì‹œê°„ = 7200ì´ˆ, ë„ˆë¬´ ê¸´ ì˜ìƒ ì œì™¸)

# ì˜¤ëŠ˜ ë‚ ì§œ ê°€ì ¸ì˜¤ê¸°
today_str = datetime.today().strftime('%Y_%m_%d')

# ë°”íƒ•í™”ë©´ì˜ youtube-ML ê²½ë¡œ ìƒì„±
BASE_FOLDER_PATH = os.path.join(os.path.expanduser("~"), 'Desktop', 'youtube-ML')
os.makedirs(BASE_FOLDER_PATH, exist_ok=True)

# ìë§‰ í´ë” ë° CSV íŒŒì¼ ê²½ë¡œ ì„¤ì •
TRANSCRIPTS_FOLDER_NAME = os.path.join(BASE_FOLDER_PATH, f'transcripts_{today_str}')
CSV_FILE_NAME = os.path.join(BASE_FOLDER_PATH, f'game_videos_{today_str}.csv')
#QUOTA_LOG_FILE = os.path.join(BASE_FOLDER_PATH, 'api_quota_usage.json')

# API í• ë‹¹ëŸ‰ ì¶”ì ì„ ìœ„í•œ ì¹´ìš´í„° ì´ˆê¸°í™”
api_quota_usage = {
    "search.list": 0,
    "videos.list": 0,
    "channels.list": 0,
    "captions.list": 0,
    "total_cost": 0
}

# í• ë‹¹ëŸ‰ ë¹„ìš© (ë‹¨ìœ„)
QUOTA_COSTS = {
    "search.list": 100,
    "videos.list": 1,
    "channels.list": 1,
    "captions.list": 50
}

# ì´ë¯¸ ìˆ˜ì§‘ëœ ë¹„ë””ì˜¤ IDë¥¼ ì €ì¥í•  ì§‘í•©
collected_video_ids = set()

# -------------------- í•¨ìˆ˜ ì •ì˜ -------------------- #
def load_existing_video_ids():
    """ê¸°ì¡´ CSV íŒŒì¼ì—ì„œ ì´ë¯¸ ìˆ˜ì§‘í•œ ì˜ìƒ IDë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    existing_ids = set()
    try:
        if os.path.exists(CSV_FILE_NAME):
            df = pd.read_csv(CSV_FILE_NAME)
            if 'ì˜ìƒID' in df.columns:
                existing_ids = set(df['ì˜ìƒID'].tolist())
                print(f"ì´ë¯¸ ìˆ˜ì§‘ëœ {len(existing_ids)}ê°œ ì˜ìƒ IDë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"ê¸°ì¡´ ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    return existing_ids

def update_quota_usage(api_method):
    """API í˜¸ì¶œ í• ë‹¹ëŸ‰ ì‚¬ìš©ëŸ‰ì„ ì—…ë°ì´íŠ¸í•˜ê³  ë¡œê·¸ì— ê¸°ë¡í•©ë‹ˆë‹¤."""
    api_quota_usage[api_method] += 1
    api_quota_usage["total_cost"] += QUOTA_COSTS[api_method]
    
    # í• ë‹¹ëŸ‰ ê²½ê³ 
    if api_quota_usage["total_cost"] > 9000:  # ì¼ì¼ í• ë‹¹ëŸ‰ 10,000ì˜ 90%
        print("âš ï¸ ê²½ê³ : API í• ë‹¹ëŸ‰ì˜ 90%ë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. ê³§ í•œë„ì— ë„ë‹¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

def get_popular_game_videos(youtube, page_token=None):
    """ì¸ê¸° ìˆëŠ” ê²Œì„ ê´€ë ¨ ì˜ìƒì„ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    try:
        # í•„ìš”í•œ í•„ë“œë§Œ ìš”ì²­í•˜ì—¬ ì‘ë‹µ í¬ê¸° ìµœì†Œí™”
        search_response = youtube.search().list(
            part="id,snippet",
            #q=SEARCH_QUERY,
            type="video",
            videoCategoryId="20",  # ê²Œì„ ì¹´í…Œê³ ë¦¬
            videoDefinition="high",  # ê³ í™”ì§ˆ ì˜ìƒ (ì¼ë°˜ì ìœ¼ë¡œ ë” ì¸ê¸° ìˆëŠ” ì˜ìƒ)
            maxResults=VIDEOS_PER_REQUEST,
            regionCode="KR",
            relevanceLanguage="ko",
            publishedAfter=PUBLISHED_AFTER_DATE,
            videoDuration="long",  # 20ë¶„ ì´ˆê³¼ ì˜ìƒë§Œ ê²€ìƒ‰
            order="viewCount",  # 'rating'ì€ ì¸ê¸°ë„ ê¸°ì¤€ ì •ë ¬ (viewCount, relevanceë„ ê³ ë ¤ ê°€ëŠ¥)
            pageToken=page_token,
            fields="nextPageToken,items(id(videoId),snippet(channelId,channelTitle,publishedAt,title))"
        ).execute()
        
        update_quota_usage("search.list")
        return search_response
    except Exception as e:
        print(f"âŒ ì˜ìƒ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        time.sleep(5)  # API ì˜¤ë¥˜ ì‹œ ì ì‹œ ëŒ€ê¸°
        return {"items": []}

def get_videos_details(youtube, video_ids):
    """ì—¬ëŸ¬ ë¹„ë””ì˜¤ IDì— ëŒ€í•œ ìƒì„¸ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    if not video_ids:
        return []
    
    try:
        # í•„ìš”í•œ í•„ë“œë§Œ ìš”ì²­
        video_response = youtube.videos().list(
            part="snippet,contentDetails,statistics",
            id=",".join(video_ids),
            fields="items(id,snippet(title,description,publishedAt,tags,thumbnails/high/url,channelId,channelTitle),contentDetails(duration,caption),statistics(viewCount,likeCount,commentCount))"
        ).execute()
        
        update_quota_usage("videos.list")
        return video_response.get("items", [])
    except Exception as e:
        print(f"âŒ ì˜ìƒ ìƒì„¸ ì •ë³´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        time.sleep(3)
        return []

def get_channel_stats(youtube, channel_ids):
    """ì±„ë„ ID ëª©ë¡ì— ëŒ€í•œ í†µê³„ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    if not channel_ids:
        return {}
    
    unique_channel_ids = list(set(channel_ids))
    channel_stats = {}
    
    # í•œ ë²ˆì— ìš”ì²­í•  ì±„ë„ ID ìˆ˜ ì œí•œ (ìµœëŒ€ 50ê°œ)
    for i in range(0, len(unique_channel_ids), 50):
        batch_channel_ids = unique_channel_ids[i:i+50]
        try:
            channel_response = youtube.channels().list(
                part="statistics",
                id=",".join(batch_channel_ids),
                fields="items(id,statistics(subscriberCount,viewCount,videoCount))"
            ).execute()
            
            update_quota_usage("channels.list")
            
            for item in channel_response.get("items", []):
                channel_stats[item["id"]] = item.get("statistics", {})
            
            time.sleep(0.5)  # API ìš”ì²­ ê°„ ì§§ì€ ëŒ€ê¸°
        except Exception as e:
            print(f"âŒ ì±„ë„ ì •ë³´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            time.sleep(3)
    
    return channel_stats

def get_video_transcript(video_id):
    """ì˜ìƒì˜ ìë§‰ì„ ê°€ì ¸ì™€ í…ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤. ìë™ ìƒì„± ìë§‰ë„ í™œìš©í•©ë‹ˆë‹¤."""
    transcript_text = ""
    transcript_type = "none"
    
    try:
        # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  ìë§‰ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)
        
        # 1. í•œêµ­ì–´ ìˆ˜ë™ ìë§‰ ì‹œë„
        try:
            transcript = transcript_list.find_manually_created_transcript(['ko'])
            transcript_data = transcript.fetch()
            transcript_text = TextFormatter().format_transcript(transcript_data)
            transcript_type = "manual_ko"
            return transcript_text, transcript_type
        except:
            pass
        
        # 2. í•œêµ­ì–´ ìë™ ìƒì„± ìë§‰ ì‹œë„
        try:
            transcript = transcript_list.find_generated_transcript(['ko'])
            transcript_data = transcript.fetch()
            transcript_text = TextFormatter().format_transcript(transcript_data)
            transcript_type = "auto_ko"
            return transcript_text, transcript_type
        except:
            pass
        
        # 3. ì˜ì–´ ìë§‰ + ë²ˆì—­ ì‹œë„
        try:
            transcript = transcript_list.find_transcript(['en'])
            # ì˜ì–´ ìë§‰ì„ í•œêµ­ì–´ë¡œ ë²ˆì—­
            translated_transcript = transcript.translate('ko')
            transcript_data = translated_transcript.fetch()
            transcript_text = TextFormatter().format_transcript(transcript_data)
            transcript_type = "translated_ko"
            return transcript_text, transcript_type
        except:
            pass
        
        # 4. ë‹¤ë¥¸ ì–¸ì–´ ìë§‰ + ë²ˆì—­ ì‹œë„
        try:
            # ì‚¬ìš© ê°€ëŠ¥í•œ ì²« ë²ˆì§¸ ìë§‰ ì„ íƒ
            transcript = next(iter(transcript_list))
            # í•œêµ­ì–´ë¡œ ë²ˆì—­
            translated_transcript = transcript.translate('ko')
            transcript_data = translated_transcript.fetch()
            transcript_text = TextFormatter().format_transcript(transcript_data)
            transcript_type = f"translated_from_other"
            return transcript_text, transcript_type
        except:
            pass
            
    except Exception as e:
        # ìë§‰ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ëŠ” ê²½ìš°
        pass
    
    return transcript_text, transcript_type

def save_transcript_to_file(video_id, transcript_text):
    """ìë§‰ í…ìŠ¤íŠ¸ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    if not transcript_text:
        return ""
    
    try:
        transcript_file_path = os.path.join(TRANSCRIPTS_FOLDER_NAME, f"{video_id}.txt")
        with open(transcript_file_path, "w", encoding="utf-8") as f:
            f.write(transcript_text)
        return transcript_file_path
    except Exception as e:
        print(f"âŒ ìë§‰ íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return ""

def format_duration(seconds):
    """ì´ˆ ë‹¨ìœ„ ì‹œê°„ì„ 'ì‹œ:ë¶„:ì´ˆ' í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    hours = seconds // 3600
    minutes = (seconds % 3600) // 60
    seconds = seconds % 60
    
    if hours > 0:
        return f"{int(hours)}:{int(minutes):02d}:{int(seconds):02d}"
    else:
        return f"{int(minutes):02d}:{int(seconds):02d}"

def main():
    """ë©”ì¸ í•¨ìˆ˜: ìœ íŠœë¸Œ ê²Œì„ ì˜ìƒ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  CSVë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    # ìë§‰ ì €ì¥ í´ë” ìƒì„±
    os.makedirs(TRANSCRIPTS_FOLDER_NAME, exist_ok=True)
    
    collected_video_ids = load_existing_video_ids()
    try:
        # YouTube API ì„œë¹„ìŠ¤ ê°ì²´ ìƒì„±
        youtube = build('youtube', 'v3', developerKey=API_KEY)
    except Exception as e:
        print(f"âŒ YouTube API ì„œë¹„ìŠ¤ ê°ì²´ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return
    
    video_data = []
    next_page_token = None
    total_processed = 0
    successfully_collected = 0
    
    print(f"ğŸ® '{SEARCH_QUERY}' ê´€ë ¨ ì¸ê¸° ê²Œì„ ì˜ìƒ ìˆ˜ì§‘ ì‹œì‘ (ëª©í‘œ: {MAX_TOTAL_VIDEOS_TO_COLLECT}ê°œ)")
    print(f"ğŸ“… {PUBLISHED_AFTER_DATE} ì´í›„ ê²Œì‹œëœ 20ë¶„ ì´ˆê³¼ ì˜ìƒ ëŒ€ìƒ")
    
    # ìˆ˜ì§‘ ì‹œì‘
    while successfully_collected < MAX_TOTAL_VIDEOS_TO_COLLECT:
        # API í• ë‹¹ëŸ‰ í™•ì¸
        if api_quota_usage["total_cost"] >= 9500:  # ì¼ì¼ í• ë‹¹ëŸ‰ 10,000ì˜ 95%
            print("âš ï¸ API í• ë‹¹ëŸ‰ì´ ê±°ì˜ ì†Œì§„ë˜ì—ˆìŠµë‹ˆë‹¤. ë°ì´í„° ìˆ˜ì§‘ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
            break
        
        # 1. ì¸ê¸° ê²Œì„ ì˜ìƒ ê²€ìƒ‰
        search_response = get_popular_game_videos(youtube, next_page_token)
        search_items = search_response.get("items", [])
        
        if not search_items:
            print("ğŸ“Œ ë” ì´ìƒ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            break
        
        # ê²€ìƒ‰ëœ ì˜ìƒ IDì™€ ì±„ë„ ID ì¶”ì¶œ
        video_ids = []
        for item in search_items:
            video_id = item["id"]["videoId"]
            if video_id not in collected_video_ids:
                video_ids.append(video_id)
                collected_video_ids.add(video_id)
        
        total_processed += len(search_items)
        
        if not video_ids:
            # ëª¨ë“  ì˜ìƒì´ ì´ë¯¸ ìˆ˜ì§‘ëœ ê²½ìš° ë‹¤ìŒ í˜ì´ì§€ë¡œ
            next_page_token = search_response.get("nextPageToken")
            if not next_page_token:
                break
            continue
        
        # 2. ì˜ìƒ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        video_details = get_videos_details(youtube, video_ids)
        
        if not video_details:
            # ìƒì„¸ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í•œ ê²½ìš° ë‹¤ìŒ í˜ì´ì§€ë¡œ
            next_page_token = search_response.get("nextPageToken")
            if not next_page_token:
                break
            continue
        
        # ì±„ë„ ID ìˆ˜ì§‘
        channel_ids = [item.get("snippet", {}).get("channelId") for item in video_details if "snippet" in item]
        
        # 3. ì±„ë„ í†µê³„ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        channel_stats = get_channel_stats(youtube, channel_ids)
        
        # 4. ì˜ìƒë³„ ì²˜ë¦¬
        for video_item in video_details:
            if successfully_collected >= MAX_TOTAL_VIDEOS_TO_COLLECT:
                break

            content_details = video_item.get("contentDetails", {})
            duration_iso = content_details.get("duration", "PT0S")
            duration_seconds = isodate.parse_duration(duration_iso).total_seconds()

            if duration_seconds < MINIMUM_VIDEO_DURATION_SECONDS or duration_seconds > MAX_VIDEO_DURATION_SECONDS:
                continue

            video_id = video_item["id"]
            snippet = video_item.get("snippet", {})
            title = snippet.get('title', '')
            description = snippet.get('description', '')

            # í•œê¸€ í¬í•¨ ì—¬ë¶€ í™•ì¸
            has_korean = any(ord('ê°€') <= ord(char) <= ord('í£') for char in title + description)
            if not has_korean:
                continue

            channel_id = snippet.get("channelId", "")
            
            # ğŸ’¡ ì—¬ê¸°ì„œ í•œêµ­ ì±„ë„ ì—¬ë¶€ í™•ì¸
            try:
                channels_response = youtube.channels().list(
                    part="snippet",
                    id=channel_id,
                    fields="items(snippet(country))"
                ).execute()
                update_quota_usage("channels.list")
                country = channels_response['items'][0]['snippet'].get('country', '')
                if country != 'KR':
                    continue
            except Exception as e:
                print(f"âŒ ì±„ë„ êµ­ê°€ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
                continue

            stats = video_item.get("statistics", {})
            channel_stat_info = channel_stats.get(channel_id, {})
            
            # 5. ìë§‰ ê°€ì ¸ì˜¤ê¸°
            transcript_text, transcript_type = get_video_transcript(video_id)
            transcript_file_path = save_transcript_to_file(video_id, transcript_text)
            
            # 6. ë°ì´í„° ì €ì¥
            video_data.append({
                'ì˜ìƒID': video_id,
                'ì˜ìƒì œëª©': snippet.get('title', ''),
                'ì˜ìƒì„¤ëª…': snippet.get('description', ''),
                'ê²Œì‹œì¼': snippet.get('publishedAt', ''),
                'íƒœê·¸': ', '.join(snippet.get('tags', [])),
                'ì¹´í…Œê³ ë¦¬ID': snippet.get('categoryId', ''),
                'ì¸ë„¤ì¼URL': snippet.get('thumbnails', {}).get('high', {}).get('url', ''),
                'ì¡°íšŒìˆ˜': int(stats.get('viewCount', 0)),
                'ì¢‹ì•„ìš”ìˆ˜': int(stats.get('likeCount', 0)),
                'ëŒ“ê¸€ìˆ˜': int(stats.get('commentCount', 0)),
                'ì˜ìƒê¸¸ì´(ì´ˆ)': int(duration_seconds),
                'ì˜ìƒê¸¸ì´(í‘œì‹œ)': format_duration(duration_seconds),
                'ìë§‰ì—¬ë¶€': content_details.get('caption', 'false'),
                'ì±„ë„ID': channel_id,
                'ì±„ë„ëª…': snippet.get('channelTitle', ''),
                'êµ¬ë…ììˆ˜': int(channel_stat_info.get('subscriberCount', 0)),
                'ì±„ë„ì´ì¡°íšŒìˆ˜': int(channel_stat_info.get('viewCount', 0)),
                'ì±„ë„ì—…ë¡œë“œì˜ìƒìˆ˜': int(channel_stat_info.get('videoCount', 0)),
                'ìë§‰ìœ í˜•': transcript_type,
            })
            
            successfully_collected += 1
            print(f"âœ… [{successfully_collected}/{MAX_TOTAL_VIDEOS_TO_COLLECT}] ìˆ˜ì§‘: {snippet.get('title', '')[:40]}... (ê¸¸ì´: {format_duration(duration_seconds)}, ì¡°íšŒìˆ˜: {stats.get('viewCount', '0')})")
        
        # ë‹¤ìŒ í˜ì´ì§€ í† í° ì„¤ì •
        next_page_token = search_response.get("nextPageToken")
        if not next_page_token:
            print("ğŸ“Œ ëª¨ë“  ê²€ìƒ‰ ê²°ê³¼ë¥¼ í™•ì¸í–ˆìŠµë‹ˆë‹¤.")
            break
        
        # API ìš”ì²­ ê°„ ëŒ€ê¸° ì‹œê°„ (ë¬´ì‘ìœ„ë¡œ ì„¤ì •í•˜ì—¬ API ì œí•œ ë°©ì§€)
        wait_time = random.uniform(1.0, 2.5)
        print(f"--- í˜„ì¬ê¹Œì§€ {successfully_collected}ê°œ ìˆ˜ì§‘, ë‹¤ìŒ í˜ì´ì§€ ìš”ì²­ ëŒ€ê¸° ({wait_time:.1f}ì´ˆ) ---")
        time.sleep(wait_time)
    
    # 7. CSV íŒŒì¼ë¡œ ì €ì¥
    if video_data:
        # ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
        new_df = pd.DataFrame(video_data)
        
        # ê¸°ì¡´ CSV íŒŒì¼ì´ ìˆìœ¼ë©´ ë³‘í•©
        if os.path.exists(CSV_FILE_NAME):
            try:
                existing_df = pd.read_csv(CSV_FILE_NAME)
                # ê¸°ì¡´ ë°ì´í„°ì™€ ìƒˆ ë°ì´í„° ë³‘í•©
                combined_df = pd.concat([existing_df, new_df], ignore_index=True)
                # ì˜ìƒ ID ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±° (ì²« ë²ˆì§¸ í•­ëª© ìœ ì§€)
                combined_df.drop_duplicates(subset=['ì˜ìƒID'], keep='first', inplace=True)
                combined_df.to_csv(CSV_FILE_NAME, index=False, encoding='utf-8-sig')
                print(f"\nâœ… ê¸°ì¡´ {len(existing_df)}ê°œ + ìƒˆë¡œìš´ {len(new_df)}ê°œ = ì´ {len(combined_df)}ê°œ ì˜ìƒ ë°ì´í„° (ì¤‘ë³µ ì œê±° í›„)")
            except Exception as e:
                print(f"ë°ì´í„° ë³‘í•© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                # ì˜¤ë¥˜ ë°œìƒ ì‹œ ìƒˆ ë°ì´í„°ë§Œ ì €ì¥
                new_df.to_csv(CSV_FILE_NAME, index=False, encoding='utf-8-sig')
                print(f"\nâœ… ì´ {len(new_df)}ê°œì˜ ê²Œì„ ì˜ìƒ ë°ì´í„°ë¥¼ '{CSV_FILE_NAME}' íŒŒì¼ë¡œ ì €ì¥ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.")
        else:
            # ê¸°ì¡´ íŒŒì¼ì´ ì—†ìœ¼ë©´ ìƒˆë¡œ ì €ì¥
            new_df.to_csv(CSV_FILE_NAME, index=False, encoding='utf-8-sig')
            print(f"\nâœ… ì´ {len(new_df)}ê°œì˜ ê²Œì„ ì˜ìƒ ë°ì´í„°ë¥¼ '{CSV_FILE_NAME}' íŒŒì¼ë¡œ ì €ì¥ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.")
        
        print(f"âœ… ìë§‰ íŒŒì¼ì€ '{TRANSCRIPTS_FOLDER_NAME}' í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print("\nâŒ ì¡°ê±´ì— ë§ëŠ” ì˜ìƒ ë°ì´í„°ê°€ ìˆ˜ì§‘ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    # 8. API ì‚¬ìš©ëŸ‰ ìš”ì•½
    print("\nğŸ“Š API í• ë‹¹ëŸ‰ ì‚¬ìš© í˜„í™©:")
    for method, count in api_quota_usage.items():
        if method != "total_cost":
            cost = count * QUOTA_COSTS[method]
            print(f"  - {method}: {count}íšŒ í˜¸ì¶œ (ë¹„ìš©: {cost} ë‹¨ìœ„)")
    print(f"  - ì´ ì‚¬ìš©ëŸ‰: {api_quota_usage['total_cost']} / 10,000 ë‹¨ìœ„")

if __name__ == "__main__":
    main()
