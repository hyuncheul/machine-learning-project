#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
K-Gaming YouTube Collector  (4 min ~ 2 h Videos Â· 1-day windows)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
í•œêµ­ì–´ ì˜ìƒ + 1ì¼ ìœˆë„ìš° + Shorts(<4m) ì œì™¸ + ê¸¸ì´ ì»¬ëŸ¼(ì´ˆÂ·hh:mm:ss)
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ import & ê¸°ë³¸ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from googleapiclient.discovery import build
from youtube_transcript_api import YouTubeTranscriptApi
from youtube_transcript_api.formatters import TextFormatter
from dateutil import rrule, parser as dtp
import isodate, pandas as pd, os, time, random, re
from datetime import datetime, timedelta

API_KEY   = "AIzaSyDRE48qtjqYwxNnypL8nZn62qqcuIW7BZQ"           # â† ë³¸ì¸ í‚¤ ì…ë ¥
CSV_NAME  = "game_api_data.csv"
TXT_DIR   = "transcript_api"
os.makedirs(TXT_DIR, exist_ok=True)

GAME_QUERIES = {
    "ë¡¤":          "ë¡¤",
    "ì„œë“ ":        "ì„œë“ ",
    "FCì˜¨ë¼ì¸":    "í”¼íŒŒ",
    "ë°°í‹€ê·¸ë¼ìš´ë“œ": "ë°°ê·¸",
    "ë°œë¡œë€íŠ¸":    "ë°œë¡œ",
}
TOPIC_ID = {
    "ë¡¤": "/m/04n3w2r",
    "ë°œë¡œë€íŠ¸": "/g/11hcz1r8jm",
    "ë°°í‹€ê·¸ë¼ìš´ë“œ": None,
    "ì„œë“ ": None,
    "FCì˜¨ë¼ì¸": None,
}

DATE_WINDOWS = [
    (d.strftime("%Y-%m-%dT%H:%M:%SZ"),
     (d + timedelta(days=3)).strftime("%Y-%m-%dT%H:%M:%SZ"))
    for d in rrule.rrule(freq=rrule.DAILY, interval=3,
                         dtstart=dtp.parse("2024-03-01"),
                         until=dtp.parse("2024-03-28"))
]

# â–¶ ê¸¸ì´ ì œí•œ: 4 ë¶„(240 s) ~ 2 h(7200 s)
MIN_SEC, MAX_SEC = 240, 7_200

COST = {"search.list":100,"videos.list":1,"channels.list":1}
quota, LIMIT = 0, 9_500
kor_re = re.compile(r"[ê°€-í£]")
today  = datetime.today().strftime("%Y-%m-%d")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ helper í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def bump(u):
    global quota
    quota += u
    if quota >= LIMIT:
        raise RuntimeError(f"â¹ quota {quota:,}/10 000 unit ì´ˆê³¼")

def log(msg): print(f"[{datetime.now().strftime('%H:%M:%S')}] {msg}")

def existing_ids():
    if os.path.exists(CSV_NAME):
        try: return set(pd.read_csv(CSV_NAME)["ì˜ìƒID"])
        except: pass
    return set()

def sec_to_hms(sec:int):
    h, rem = divmod(sec, 3600)
    m, s   = divmod(rem, 60)
    return f"{h:02}:{m:02}:{s:02}" if h else f"{m:02}:{s:02}"

def transcript_save(vid):
    try:
        lst = YouTubeTranscriptApi.list_transcripts(vid)
        tr  = lst.find_manually_created_transcript(['ko','en']) \
              if lst._manually_created_transcripts else \
              lst.find_generated_transcript(['ko','en'])
        with open(f"{TXT_DIR}/{vid}.txt","w",encoding="utf-8") as f:
            f.write(TextFormatter().format_transcript(tr.fetch()))
        return tr.language_code
    except: return "none"

def s_list(y, **kw):
    res=y.search().list(**kw).execute(); bump(COST["search.list"]); return res
def v_list(y, ids):
    res=y.videos().list(part="snippet,contentDetails,statistics,status",
                        id=",".join(ids)).execute()
    bump(len(ids)); return res["items"]
def c_list(y, ids):
    res=y.channels().list(part="snippet,statistics,contentDetails",
                          id=",".join(ids)).execute()
    bump(len(ids)); return {i["id"]:i for i in res["items"]}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ main crawler â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    if API_KEY.startswith("YOUR_"):
        log("API_KEYë¥¼ ì…ë ¥í•˜ì„¸ìš”"); return
    yt = build("youtube","v3",developerKey=API_KEY)

    exist, records = existing_ids(), []
    try:
        for game, query in GAME_QUERIES.items():
            t_id = TOPIC_ID.get(game)
            for frm, to in DATE_WINDOWS:
                log(f"[{game}] {frm[:10]} ~ {to[:10]} ê²€ìƒ‰")
                nxt = None
                while True:
                    resp = s_list(
                        yt, part="id", q=query, type="video",
                        videoDuration="medium", 
                        #videoDuration="long", 
                        videoCategoryId="20", 
                        topicId=t_id,
                        regionCode="KR", relevanceLanguage="ko",
                        #order="videoCount", 
                        publishedAfter=frm, publishedBefore=to,
                        maxResults=50, pageToken=nxt,
                        fields="nextPageToken,items/id/videoId"
                    )
                    ids=[it["id"]["videoId"] for it in resp["items"] if it["id"]["videoId"] not in exist]
                    if not ids: break
                    det = v_list(yt, ids)

                    # ---- í•œê¸€ í¬í•¨ + ê¸¸ì´ 4m~2h í•„í„° ----
                    valid=[]
                    for d in det:
                        text = d["snippet"].get("title","")+d["snippet"].get("description","")
                        if not kor_re.search(text): continue
                        sec=int(isodate.parse_duration(d["contentDetails"]["duration"]).total_seconds())
                        if MIN_SEC<=sec<=MAX_SEC:
                            d["sec"]=sec
                            valid.append(d)
                    if not valid:
                        nxt = resp.get("nextPageToken"); 
                        if not nxt: break
                        continue

                    # ---- ì±„ë„ í†µê³„ ----
                    ch_ids = list({d["snippet"]["channelId"] for d in valid})
                    ch_map = {}
                    for j in range(0,len(ch_ids),50):
                        ch_map.update(c_list(yt,ch_ids[j:j+50]))

                    # ---- ë ˆì½”ë“œ ----
                    for v in valid:
                        sn, cd, st = v["snippet"], v["contentDetails"], v.get("statistics", {})
                        ch = ch_map.get(sn["channelId"], {})
                        sec = v["sec"]
                        records.append({
                            "ìˆ˜ì§‘ì¼ì": today,
                            "ê²Œì„ëª…": game,
                            "ì˜ìƒID": v["id"],
                            "ì˜ìƒì œëª©": sn.get("title"),
                            "ì˜ìƒì„¤ëª…": sn.get("description"),
                            "ê²Œì‹œì¼": sn.get("publishedAt"),
                            "ì±„ë„ID": sn.get("channelId"),
                            "ì±„ë„ëª…": sn.get("channelTitle"),
                            "íƒœê·¸": ", ".join(sn.get("tags", [])),
                            "ì¹´í…Œê³ ë¦¬ID": sn.get("categoryId"),
                            "ì¸ë„¤ì¼URL": sn.get("thumbnails", {}).get("high", {}).get("url"),
                            "defaultLanguage": sn.get("defaultLanguage"),
                            "duration": cd.get("duration"),          # ISO-8601
                            "ì˜ìƒê¸¸ì´(ì´ˆ)": sec,
                            "ì˜ìƒê¸¸ì´(í‘œì‹œ)": sec_to_hms(sec),
                            "dimension": cd.get("dimension"),
                            "definition": cd.get("definition"),
                            "caption": cd.get("caption"),
                            "licensedContent": cd.get("licensedContent"),
                            "privacyStatus": v["status"].get("privacyStatus"),
                            "madeForKids": v["status"].get("madeForKids"),
                            "viewCount": int(st.get("viewCount", 0)),
                            "likeCount": int(st.get("likeCount", 0)),
                            "commentCount": int(st.get("commentCount", 0)),
                            "êµ¬ë…ììˆ˜": int(ch.get("statistics", {}).get("subscriberCount", 0)),
                            "ì±„ë„ì´ì¡°íšŒìˆ˜": int(ch.get("statistics", {}).get("viewCount", 0)),
                            "ì±„ë„ì—…ë¡œë“œì˜ìƒìˆ˜": int(ch.get("statistics", {}).get("videoCount", 0)),
                            "ì±„ë„ê°œì„¤ì¼": ch.get("snippet", {}).get("publishedAt"),
                            "ìë§‰ìœ í˜•": transcript_save(v["id"]),
                        })
                    exist.update(ids)
                    log(f"  +{len(valid)}ê°œ (quota {quota:,})")
                    nxt = resp.get("nextPageToken")
                    if not nxt: break
                    time.sleep(random.uniform(0.8,1.2))
    except RuntimeError as e:
        log(str(e))
    except Exception as e:
        log(f"âŒ ì˜ˆì™¸: {e}")
    finally:
        if records:
            new_df = pd.DataFrame(records)
            if os.path.exists(CSV_NAME):
                all_df = pd.concat([pd.read_csv(CSV_NAME), new_df]) \
                          .drop_duplicates("ì˜ìƒID")
            else:
                all_df = new_df
            all_df.to_csv(CSV_NAME, index=False, encoding="utf-8-sig")
            log(f"ğŸ’¾ ì €ì¥: ìƒˆ {len(new_df)} / ì´ {len(all_df)}")
        log(f"ğŸ“Š ìµœì¢… quota {quota:,} unit")

if __name__ == "__main__":
    main()
