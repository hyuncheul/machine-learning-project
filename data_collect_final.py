# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë¼ì´ë¸ŒëŸ¬ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from googleapiclient.discovery import build
from youtube_transcript_api import YouTubeTranscriptApi
from youtube_transcript_api.formatters import TextFormatter
from dateutil import rrule, parser as dtp
import isodate, pandas as pd, os, time, random, re
from datetime import datetime, timedelta

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì„¤ì •ê°’ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
API_KEY   = "YOUR_API_KEY"
CSV_NAME  = "game_api_data.csv"
TXT_DIR   = "transcript_api"
os.makedirs(TXT_DIR, exist_ok=True)

GAME_QUERIES = {
    "ë¡¤":          "ë¦¬ê·¸ì˜¤ë¸Œë ˆì „ë“œ|ë¡¤|league of legends|LOL",
    "ì„œë“ ":        "ì„œë“ ì–´íƒ|ì„œë“ ",
    "FCì˜¨ë¼ì¸":    "FCì˜¨ë¼ì¸|í”¼íŒŒì˜¨ë¼ì¸|í”¼íŒŒ|EA FC Online",
    "ë°°í‹€ê·¸ë¼ìš´ë“œ": "ë°°í‹€ê·¸ë¼ìš´ë“œ|ë°°ê·¸|PUBG",
    "ë°œë¡œë€íŠ¸":    "ë°œë¡œë€íŠ¸|ë°œë¡œ|VALORANT",
}
TOPIC_ID = {
    "ë¡¤": "/m/04n3w2r",
    "ë°œë¡œë€íŠ¸": "/g/11hcz1r8jm",
    "ë°°í‹€ê·¸ë¼ìš´ë“œ": None,
    "ì„œë“ ": None,
    "FCì˜¨ë¼ì¸": None,
}

# ğŸ‘‰ 2024-02-01 ~ 02-29  3ì¼ ê°„ê²©
DATE_WINDOWS = [
    (d.strftime("%Y-%m-%dT%H:%M:%SZ"),
     (d + timedelta(days=3)).strftime("%Y-%m-%dT%H:%M:%SZ"))
    for d in rrule.rrule(freq=rrule.DAILY, interval=3,
                         dtstart=dtp.parse("2024-02-01"),
                         until=dtp.parse("2024-02-29"))
]

# ê¸¸ì´ ì œí•œ (20 ë¶„ = 1 200 s  ~  2 h = 7 200 s)
MIN_SEC, MAX_SEC = 1_200, 7_200

# í• ë‹¹ëŸ‰
COST = {"search.list": 100, "videos.list": 1, "channels.list": 1}
quota, LIMIT = 0, 9_500
kor_regex = re.compile(r"[ê°€-í£]")
today = datetime.today().strftime("%Y-%m-%d")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ í—¬í¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def bump(u):  # unit ëˆ„ì 
    global quota
    quota += u
    if quota >= LIMIT:
        raise RuntimeError(f"â¹ quota {quota:,}/10 000 unit ë„ë‹¬ â€“ ì¤‘ë‹¨")

def log(msg): print(f"[{datetime.now().strftime('%H:%M:%S')}] {msg}")

def existing_ids():
    if os.path.exists(CSV_NAME):
        try:
            return set(pd.read_csv(CSV_NAME)["ì˜ìƒID"])
        except Exception:
            pass
    return set()

def transcript_save(v_id):
    try:
        lst = YouTubeTranscriptApi.list_transcripts(v_id)
        tr = lst.find_manually_created_transcript(['ko', 'en']) \
             if lst._manually_created_transcripts else \
             lst.find_generated_transcript(['ko', 'en'])
        txt = TextFormatter().format_transcript(tr.fetch())
        with open(f"{TXT_DIR}/{v_id}.txt", "w", encoding="utf-8") as f:
            f.write(txt)
        return tr.language_code
    except Exception:
        return "none"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ API ë˜í¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def s_list(y, **kw):
    res = y.search().list(**kw).execute()
    bump(COST["search.list"])
    return res

def v_list(y, ids):
    res = y.videos().list(
        part="snippet,contentDetails,statistics,status",
        id=",".join(ids)
    ).execute()
    bump(len(ids))
    return res["items"]

def c_list(y, ids):
    res = y.channels().list(
        part="snippet,statistics,contentDetails",
        id=",".join(ids)
    ).execute()
    bump(len(ids))
    return {i["id"]: i for i in res["items"]}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ MAIN â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    if not API_KEY or API_KEY.startswith("YOUR_"):
        log("API_KEYë¥¼ ì…ë ¥í•˜ì„¸ìš”"); return
    yt = build("youtube", "v3", developerKey=API_KEY)

    exist_ids = existing_ids()
    rows = []

    try:
        for game, q in GAME_QUERIES.items():
            t_id = TOPIC_ID.get(game)
            for after, before in DATE_WINDOWS:
                log(f"[{game}] {after[:10]} ~ {before[:10]} ê²€ìƒ‰")
                next_tok = None
                while True:
                    resp = s_list(
                        yt,
                        part="id",
                        q=q,
                        type="video",
                        videoDuration="long",    # â˜… Shorts & medium ì°¨ë‹¨
                        videoCategoryId="20",
                        topicId=t_id,
                        regionCode="KR",
                        relevanceLanguage="ko",
                        order="date",
                        publishedAfter=after,
                        publishedBefore=before,
                        maxResults=50,
                        pageToken=next_tok,
                        fields="nextPageToken,items/id/videoId",
                    )
                    ids = [i["id"]["videoId"] for i in resp["items"]
                           if i["id"]["videoId"] not in exist_ids]
                    if not ids:
                        break

                    details = v_list(yt, ids)

                    # -------- 2ì°¨ í•„í„°: í•œê¸€ í¬í•¨ + ê¸¸ì´ 20 min~2 h -------------
                    filtered = []
                    for d in details:
                        txt = (d["snippet"].get("title", "") +
                               d["snippet"].get("description", ""))
                        if not kor_regex.search(txt):
                            continue
                        seconds = isodate.parse_duration(
                            d["contentDetails"]["duration"]).total_seconds()
                        if MIN_SEC <= seconds <= MAX_SEC:
                            filtered.append(d)
                    if not filtered:
                        next_tok = resp.get("nextPageToken")
                        if not next_tok:
                            break
                        continue

                    # -------- ì±„ë„ í†µê³„ --------
                    ch_ids = list({d["snippet"]["channelId"] for d in filtered})
                    ch_map = {}
                    for j in range(0, len(ch_ids), 50):
                        ch_map.update(c_list(yt, ch_ids[j:j+50]))

                    # -------- ë ˆì½”ë“œ ì €ì¥ --------
                    for v in filtered:
                        sn, cd, st = v["snippet"], v["contentDetails"], v.get("statistics", {})
                        ch = ch_map.get(sn["channelId"], {})
                        rows.append({
                            "ìˆ˜ì§‘ì¼ì": today,
                            "ê²Œì„ëª…": game,
                            "ì˜ìƒID": v["id"],
                            "ì˜ìƒì œëª©": sn.get("title"),
                            "ì˜ìƒì„¤ëª…": sn.get("description"),
                            "ê²Œì‹œì¼": sn.get("publishedAt"),
                            "ì±„ë„ID": sn.get("channelId"),
                            "ì±„ë„ëª…": sn.get("channelTitle"),
                            "íƒœê·¸": ", ".join(sn.get("tags", [])),
                            "ì¹´í…Œê³ ë¦¬ID": sn.get("categoryId"),
                            "ì¸ë„¤ì¼URL": sn.get("thumbnails", {}).get("high", {}).get("url"),
                            "defaultLanguage": sn.get("defaultLanguage"),
                            **{k: cd.get(k) for k in ("duration", "dimension", "definition",
                                                     "caption", "licensedContent")},
                            "privacyStatus": v["status"].get("privacyStatus"),
                            "madeForKids": v["status"].get("madeForKids"),
                            "viewCount": int(st.get("viewCount", 0)),
                            "likeCount": int(st.get("likeCount", 0)),
                            "commentCount": int(st.get("commentCount", 0)),
                            "êµ¬ë…ììˆ˜": int(ch.get("statistics", {}).get("subscriberCount", 0)),
                            "ì±„ë„ì´ì¡°íšŒìˆ˜": int(ch.get("statistics", {}).get("viewCount", 0)),
                            "ì±„ë„ì—…ë¡œë“œì˜ìƒìˆ˜": int(ch.get("statistics", {}).get("videoCount", 0)),
                            "ì±„ë„ê°œì„¤ì¼": ch.get("snippet", {}).get("publishedAt"),
                            "ìë§‰ìœ í˜•": transcript_save(v["id"]),
                        })
                    exist_ids.update(ids)
                    log(f"  +{len(filtered)}í¸  (quota {quota:,})")

                    next_tok = resp.get("nextPageToken")
                    if not next_tok:
                        break
                    time.sleep(random.uniform(0.8, 1.3))

    except RuntimeError as e:
        log(str(e))
    except Exception as e:
        log(f"âŒ ì˜ˆì™¸ ë°œìƒ: {e}")
    finally:
        if rows:
            new_df = pd.DataFrame(rows)
            if os.path.exists(CSV_NAME):
                df_all = pd.concat([pd.read_csv(CSV_NAME), new_df]) \
                          .drop_duplicates("ì˜ìƒID")
            else:
                df_all = new_df
            df_all.to_csv(CSV_NAME, index=False, encoding="utf-8-sig")
            log(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: ìƒˆ {len(new_df)} / ì´ {len(df_all)}")
        log(f"ğŸ“Š ìµœì¢… quota ì‚¬ìš©ëŸ‰: {quota:,} unit")

if __name__ == "__main__":
    main()
