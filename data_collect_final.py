from googleapiclient.discovery import build
from youtube_transcript_api import YouTubeTranscriptApi
from youtube_transcript_api.formatters import TextFormatter
from dateutil import rrule, parser as dtp
import pandas as pd, os, time, random, sys, re
from datetime import datetime, timedelta

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
API_KEY = ""            # âš ï¸ ì…ë ¥
CSV_NAME, TXT_DIR = "game_api_data.csv", "transcript_api"
os.makedirs(TXT_DIR, exist_ok=True)

# OR í‚¤ì›Œë“œ
GAME_QUERIES = {
    "ë¡¤":          "ë¦¬ê·¸ì˜¤ë¸Œë ˆì „ë“œ|ë¡¤|league of legends|LOL",
    "ì„œë“ ":        "ì„œë“ ì–´íƒ|ì„œë“ ",
    "FCì˜¨ë¼ì¸":    "FCì˜¨ë¼ì¸|í”¼íŒŒì˜¨ë¼ì¸|í”¼íŒŒ|EA FC Online",
    "ë°°í‹€ê·¸ë¼ìš´ë“œ": "ë°°í‹€ê·¸ë¼ìš´ë“œ|ë°°ê·¸|PUBG",
    "ë°œë¡œë€íŠ¸":    "ë°œë¡œë€íŠ¸|ë°œë¡œ|VALORANT"
}
TOPIC_ID = {
    "ë¡¤": "/m/04n3w2r",          # League of Legends
    "ë°œë¡œë€íŠ¸": "/g/11hcz1r8jm",  # VALORANT (KG ID)
    "ë°°í‹€ê·¸ë¼ìš´ë“œ": None,        # PUBG
    "ì„œë“ ": None,               # Sudden Attack
    "FCì˜¨ë¼ì¸": None            # EA Sports FC Online
}
# 3 ì¼ ê°„ê²© ìœˆë„ìš° ìƒì„±
DATE_WINDOWS = [(d.strftime("%Y-%m-%dT%H:%M:%SZ"),
                 (d + timedelta(days=3)).strftime("%Y-%m-%dT%H:%M:%SZ"))
                for d in rrule.rrule(freq=rrule.DAILY, interval=3,
                                    dtstart=dtp.parse("2024-01-01"),
                                    until=dtp.parse("2024-01-31"))]

COST = {"search.list":100, "videos.list":1, "channels.list":1}
quota, LIMIT = 0, 9_500
today = datetime.today().strftime("%Y-%m-%d")
kor_regex = re.compile(r"[ê°€-í£]")   # í•œê¸€ ì—¬ë¶€ ì²´í¬

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë³´ì¡° í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
def bump(u:int):
    global quota; quota += u
    if quota >= LIMIT: raise RuntimeError(f"â¹ quota {quota:,}/10 000")

def log(m): print(f"[{datetime.now().strftime('%H:%M:%S')}] {m}")

def existing_ids():
    if os.path.exists(CSV_NAME):
        try: return set(pd.read_csv(CSV_NAME)["ì˜ìƒID"])
        except: return set()
    return set()

def save(df_new):
    if df_new.empty: return
    if os.path.exists(CSV_NAME):
        df = pd.concat([pd.read_csv(CSV_NAME), df_new]).drop_duplicates("ì˜ìƒID")
    else: df = df_new
    df.to_csv(CSV_NAME, index=False, encoding="utf-8-sig")
    log(f"ğŸ’¾ ì €ì¥: ìƒˆ {len(df_new)} / ì´ {len(df)}")

def transcript_save(v_id):
    try:
        lst = YouTubeTranscriptApi.list_transcripts(v_id)
        tr = lst.find_manually_created_transcript(['ko','en']) \
             if lst._manually_created_transcripts else \
             lst.find_generated_transcript(['ko','en'])
        txt = TextFormatter().format_transcript(tr.fetch())
        with open(f"{TXT_DIR}/{v_id}.txt","w",encoding="utf-8") as f: f.write(txt)
        return tr.language_code
    except: return "none"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ API ë˜í¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
def s_list(y, **kw):
    r = y.search().list(**kw).execute(); bump(COST["search.list"]); return r
def v_list(y, ids):
    r = y.videos().list(part="snippet,contentDetails,statistics,status",
                        id=",".join(ids)).execute()
    bump(len(ids)); return r["items"]
def c_list(y, ids):
    r = y.channels().list(part="snippet,statistics,contentDetails",
                          id=",".join(ids)).execute()
    bump(len(ids)); return {i["id"]:i for i in r["items"]}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ MAIN â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
def main():
    if not API_KEY or API_KEY.startswith("YOUR_"):
        log("API_KEY ë¨¼ì € ì…ë ¥"); return
    yt = build("youtube","v3",developerKey=API_KEY)
    exist = existing_ids(); rows=[]
    try:
        for game, q in GAME_QUERIES.items():
            t_id = TOPIC_ID.get(game)
            for af, bf in DATE_WINDOWS:
                log(f"[{game}] {af[:10]}~{bf[:10]} ê²€ìƒ‰")
                nxt=None
                while True:
                    rsp=s_list(yt, part="id", q=q, type="video",
                               videoCategoryId="20", topicId=t_id,
                               regionCode="KR", relevanceLanguage="ko",
                               order="date", publishedAfter=af, publishedBefore=bf,
                               maxResults=50, pageToken=nxt,
                               fields="nextPageToken,items/id/videoId")
                    ids=[i["id"]["videoId"] for i in rsp["items"] if i["id"]["videoId"] not in exist]
                    if not ids: break
                    det=v_list(yt, ids)
                    # --- 2ì°¨: í•œê¸€ í¬í•¨ í•„í„° ---
                    det=[d for d in det if kor_regex.search(d["snippet"].get("title","")+
                                                            d["snippet"].get("description",""))]
                    if not det: nxt=rsp.get("nextPageToken"); 
                    if not nxt and not det: break
                    ch_ids=list({d["snippet"]["channelId"] for d in det})
                    ch_map={}
                    for j in range(0,len(ch_ids),50): ch_map.update(c_list(yt,ch_ids[j:j+50]))
                    for v in det:
                        sn, cd, st=v["snippet"],v["contentDetails"],v.get("statistics",{})
                        ch=ch_map.get(sn["channelId"],{})
                        rows.append({
                            "ìˆ˜ì§‘ì¼ì":today,"ê²Œì„ëª…":game,"ì˜ìƒID":v["id"],
                            "ì˜ìƒì œëª©":sn.get("title"),"ì˜ìƒì„¤ëª…":sn.get("description"),
                            "ê²Œì‹œì¼":sn.get("publishedAt"),
                            "ì±„ë„ID":sn.get("channelId"),"ì±„ë„ëª…":sn.get("channelTitle"),
                            "íƒœê·¸":", ".join(sn.get("tags",[])),"ì¹´í…Œê³ ë¦¬ID":sn.get("categoryId"),
                            "ì¸ë„¤ì¼URL":sn.get("thumbnails",{}).get("high",{}).get("url"),
                            "defaultLanguage":sn.get("defaultLanguage"),
                            **{k:cd.get(k) for k in("duration","dimension","definition",
                                                    "caption","licensedContent")},
                            "privacyStatus":v["status"].get("privacyStatus"),
                            "madeForKids":v["status"].get("madeForKids"),
                            "viewCount":int(st.get("viewCount",0)),
                            "likeCount":int(st.get("likeCount",0)),
                            "commentCount":int(st.get("commentCount",0)),
                            "êµ¬ë…ììˆ˜":int(ch.get("statistics",{}).get("subscriberCount",0)),
                            "ì±„ë„ì´ì¡°íšŒìˆ˜":int(ch.get("statistics",{}).get("viewCount",0)),
                            "ì±„ë„ì—…ë¡œë“œì˜ìƒìˆ˜":int(ch.get("statistics",{}).get("videoCount",0)),
                            "ì±„ë„ê°œì„¤ì¼":ch.get("snippet",{}).get("publishedAt"),
                            "ìë§‰ìœ í˜•":transcript_save(v["id"])
                        })
                    exist.update(ids)
                    log(f"  +{len(det)} (quota {quota:,})")
                    nxt=rsp.get("nextPageToken")
                    if not nxt: break
                    time.sleep(random.uniform(0.8,1.3))
    except RuntimeError as e:
        log(str(e))
    except Exception as e:
        log(f"âŒ ì˜ˆì™¸: {e}")
    finally:
        save(pd.DataFrame(rows))
        log(f"ğŸ“Š ìµœì¢… ì‚¬ìš©ëŸ‰ {quota:,} unit")

if __name__=="__main__":
    main()