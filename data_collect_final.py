#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
K-Gaming YouTube Collector (20 min ~ 2 h Long Videos)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
í•œêµ­ ì˜ìƒ + 3ì¼ ìœˆë„ìš° + Shorts/Medium ì œì™¸ + ê¸¸ì´ ì»¬ëŸ¼ 2ì¢… ì¶”ê°€
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ import & config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from googleapiclient.discovery import build
from youtube_transcript_api import YouTubeTranscriptApi
from youtube_transcript_api.formatters import TextFormatter
from dateutil import rrule, parser as dtp
import isodate, pandas as pd, os, time, random, re
from datetime import datetime, timedelta

API_KEY   = "YOUR_API_KEY"                 # â† ë³¸ì¸ í‚¤ ìž…ë ¥
CSV_NAME  = "game_api_data.csv"
TXT_DIR   = "transcript_api"
os.makedirs(TXT_DIR, exist_ok=True)

GAME_QUERIES = {
    "ë¡¤":          "ë¦¬ê·¸ì˜¤ë¸Œë ˆì „ë“œ|ë¡¤|league of legends|LOL",
    "ì„œë“ ":        "ì„œë“ ì–´íƒ|ì„œë“ ",
    "FCì˜¨ë¼ì¸":    "FCì˜¨ë¼ì¸|í”¼íŒŒì˜¨ë¼ì¸|í”¼íŒŒ|EA FC Online",
    "ë°°í‹€ê·¸ë¼ìš´ë“œ": "ë°°í‹€ê·¸ë¼ìš´ë“œ|ë°°ê·¸|PUBG",
    "ë°œë¡œëž€íŠ¸":    "ë°œë¡œëž€íŠ¸|ë°œë¡œ|VALORANT",
}
TOPIC_ID = {
    "ë¡¤": "/m/04n3w2r",
    "ë°œë¡œëž€íŠ¸": "/g/11hcz1r8jm",
    "ë°°í‹€ê·¸ë¼ìš´ë“œ": None,
    "ì„œë“ ": None,
    "FCì˜¨ë¼ì¸": None,
}

DATE_WINDOWS = [
    (d.strftime("%Y-%m-%dT%H:%M:%SZ"),
     (d + timedelta(days=3)).strftime("%Y-%m-%dT%H:%M:%SZ"))
    for d in rrule.rrule(freq=rrule.DAILY, interval=3,
                         dtstart=dtp.parse("2024-02-01"),
                         until=dtp.parse("2024-02-29"))
]

MIN_SEC, MAX_SEC = 1_200, 7_200           # 20ë¶„-2ì‹œê°„
COST, quota, LIMIT = {"search.list":100,"videos.list":1,"channels.list":1}, 0, 9_500
kor_re  = re.compile(r"[ê°€-íž£]")
today   = datetime.today().strftime("%Y-%m-%d")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def bump(u):  # quota ëˆ„ì 
    global quota
    quota += u
    if quota >= LIMIT:
        raise RuntimeError(f"â¹ quota {quota:,}/10 000 unit ë„ë‹¬")

def log(m): print(f"[{datetime.now().strftime('%H:%M:%S')}] {m}")

def existing_ids():
    if os.path.exists(CSV_NAME):
        try: return set(pd.read_csv(CSV_NAME)["ì˜ìƒID"])
        except: pass
    return set()

def hms(sec:int) -> str:
    h, m = divmod(sec, 3600)
    m, s = divmod(m, 60)
    return f"{h:02}:{m:02}:{s:02}" if h else f"{m:02}:{s:02}"

def transcript_save(vid):
    try:
        lst = YouTubeTranscriptApi.list_transcripts(vid)
        tr  = lst.find_manually_created_transcript(['ko','en']) \
              if lst._manually_created_transcripts else \
              lst.find_generated_transcript(['ko','en'])
        with open(f"{TXT_DIR}/{vid}.txt","w",encoding="utf-8") as f:
            f.write(TextFormatter().format_transcript(tr.fetch()))
        return tr.language_code
    except: return "none"

def s_list(y, **k):
    r=y.search().list(**k).execute(); bump(COST["search.list"]); return r
def v_list(y, ids):
    r=y.videos().list(part="snippet,contentDetails,statistics,status",
                      id=",".join(ids)).execute()
    bump(len(ids)); return r["items"]
def c_list(y, ids):
    r=y.channels().list(part="snippet,statistics,contentDetails",
                        id=",".join(ids)).execute()
    bump(len(ids)); return {i["id"]:i for i in r["items"]}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ main crawler â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    if API_KEY.startswith("YOUR_"):
        log("API_KEYë¥¼ ìž…ë ¥í•˜ì„¸ìš”"); return
    yt = build("youtube","v3",developerKey=API_KEY)

    exist = existing_ids(); records=[]
    try:
        for game,q in GAME_QUERIES.items():
            t_id = TOPIC_ID.get(game)
            for fr,to in DATE_WINDOWS:
                log(f"[{game}] {fr[:10]}~{to[:10]} ê²€ìƒ‰")
                nxt=None
                while True:
                    rsp=s_list(yt, part="id", q=q, type="video",
                               videoDuration="long", videoCategoryId="20",
                               topicId=t_id, regionCode="KR", relevanceLanguage="ko",
                               order="date", publishedAfter=fr, publishedBefore=to,
                               maxResults=50, pageToken=nxt,
                               fields="nextPageToken,items/id/videoId")
                    ids=[i["id"]["videoId"] for i in rsp["items"] if i["id"]["videoId"] not in exist]
                    if not ids: break
                    det=v_list(yt, ids)

                    kept=[]
                    for d in det:
                        txt= d["snippet"].get("title","")+d["snippet"].get("description","")
                        if not kor_re.search(txt): continue
                        sec=int(isodate.parse_duration(d["contentDetails"]["duration"]).total_seconds())
                        if MIN_SEC<=sec<=MAX_SEC:
                            d["sec"]=sec
                            kept.append(d)
                    if not kept:
                        nxt=rsp.get("nextPageToken"); 
                        if not nxt: break
                        continue

                    ch_ids=list({k["snippet"]["channelId"] for k in kept})
                    ch_map={}
                    for j in range(0,len(ch_ids),50):
                        ch_map.update(c_list(yt,ch_ids[j:j+50]))

                    for v in kept:
                        sn,cd,st=v["snippet"],v["contentDetails"],v.get("statistics",{})
                        ch=ch_map.get(sn["channelId"],{})
                        sec=v["sec"]
                        records.append({
                            "ìˆ˜ì§‘ì¼ìž":today,"ê²Œìž„ëª…":game,"ì˜ìƒID":v["id"],
                            "ì˜ìƒì œëª©":sn.get("title"),"ì˜ìƒì„¤ëª…":sn.get("description"),
                            "ê²Œì‹œì¼":sn.get("publishedAt"),
                            "ì±„ë„ID":sn.get("channelId"),"ì±„ë„ëª…":sn.get("channelTitle"),
                            "íƒœê·¸":", ".join(sn.get("tags",[])),"ì¹´í…Œê³ ë¦¬ID":sn.get("categoryId"),
                            "ì¸ë„¤ì¼URL":sn.get("thumbnails",{}).get("high",{}).get("url"),
                            "defaultLanguage":sn.get("defaultLanguage"),
                            "duration":cd.get("duration"),          # ISO-8601
                            "ì˜ìƒê¸¸ì´(ì´ˆ)":sec,
                            "ì˜ìƒê¸¸ì´(í‘œì‹œ)":hms(sec),
                            "dimension":cd.get("dimension"),
                            "definition":cd.get("definition"),
                            "caption":cd.get("caption"),
                            "licensedContent":cd.get("licensedContent"),
                            "privacyStatus":v["status"].get("privacyStatus"),
                            "madeForKids":v["status"].get("madeForKids"),
                            "viewCount":int(st.get("viewCount",0)),
                            "likeCount":int(st.get("likeCount",0)),
                            "commentCount":int(st.get("commentCount",0)),
                            "êµ¬ë…ìžìˆ˜":int(ch.get("statistics",{}).get("subscriberCount",0)),
                            "ì±„ë„ì´ì¡°íšŒìˆ˜":int(ch.get("statistics",{}).get("viewCount",0)),
                            "ì±„ë„ì—…ë¡œë“œì˜ìƒìˆ˜":int(ch.get("statistics",{}).get("videoCount",0)),
                            "ì±„ë„ê°œì„¤ì¼":ch.get("snippet",{}).get("publishedAt"),
                            "ìžë§‰ìœ í˜•":transcript_save(v["id"])
                        })
                    exist.update(ids)
                    log(f"  +{len(kept)}ê°œ (quota {quota:,})")
                    nxt=rsp.get("nextPageToken")
                    if not nxt: break
                    time.sleep(random.uniform(0.8,1.3))
    except RuntimeError as e:
        log(str(e))
    except Exception as e:
        log(f"âŒ ì˜ˆì™¸: {e}")
    finally:
        if records:
            new_df=pd.DataFrame(records)
            if os.path.exists(CSV_NAME):
                df_all=pd.concat([pd.read_csv(CSV_NAME),new_df])\
                       .drop_duplicates("ì˜ìƒID")
            else: df_all=new_df
            df_all.to_csv(CSV_NAME,index=False,encoding="utf-8-sig")
            log(f"ðŸ’¾ ì €ìž¥: ìƒˆ {len(new_df)} / ì´ {len(df_all)}")
        log(f"ðŸ“Š ìµœì¢… quota {quota:,} unit")

if __name__=="__main__":
    main()
