from googleapiclient.discovery import build
from youtube_transcript_api import YouTubeTranscriptApi
from youtube_transcript_api.formatters import TextFormatter
import isodate
import pandas as pd
import os
import time
from datetime import datetime
import json
import random

# -------------------- ì„¤ì • ê°’ -------------------- #
API_KEY = '' # ì—¬ê¸°ì— ë³¸ì¸ì˜ API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”.
if not API_KEY:
    print("ì˜¤ë¥˜: API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    exit()

SEARCH_KEYWORDS = [
    'ë¡¤', 'ë¦¬ê·¸ì˜¤ë¸Œë ˆì „ë“œ', 'ë§ˆì¸í¬ë˜í”„íŠ¸', 'ë°œë¡œë€íŠ¸', 
    'FCì˜¨ë¼ì¸', 'ì˜¤ë²„ì›Œì¹˜', 'ë°°í‹€ê·¸ë¼ìš´ë“œ', 'ë˜ì „ì•¤íŒŒì´í„°',
    'ì„œë“ ì–´íƒ', 'ë¡œìŠ¤íŠ¸ì•„í¬', 'ë©”ì´í”ŒìŠ¤í† ë¦¬', 'ìŠ¤íƒ€í¬ë˜í”„íŠ¸'
]
MAX_TOTAL_VIDEOS_TO_COLLECT = 2000
VIDEOS_PER_REQUEST = 50
PUBLISHED_AFTER_DATE = "2024-10-01T00:00:00Z"
PUBLISHED_BEFORE_DATE = "2025-01-01T00:00:00Z"
MINIMUM_VIDEO_DURATION_SECONDS = 480  # 8ë¶„
MAX_VIDEO_DURATION_SECONDS = 7200     # 2ì‹œê°„

# ì˜¤ëŠ˜ ë‚ ì§œ ê°€ì ¸ì˜¤ê¸°
today_str = datetime.today().strftime('%Y_%m_%d')

# í˜„ì¬ íŒŒì¼ì´ ìˆëŠ” í´ë” ê²½ë¡œë¥¼ BASE_FOLDER_PATHë¡œ ì„¤ì •
BASE_FOLDER_PATH = os.path.dirname(os.path.abspath(__file__))
os.makedirs(BASE_FOLDER_PATH, exist_ok=True)

# ìë§‰ í´ë” ê²½ë¡œë¥¼ BASE_FOLDER_PATH í•˜ìœ„ì˜ transcripts_apië¡œ ì„¤ì •
TRANSCRIPTS_FOLDER_NAME = os.path.join(BASE_FOLDER_PATH, 'transcripts_api')
os.makedirs(TRANSCRIPTS_FOLDER_NAME, exist_ok=True)

# CSV íŒŒì¼ ê²½ë¡œ ì„¤ì •
CSV_FILE_NAME = os.path.join(BASE_FOLDER_PATH, 'game_api_data.csv')

# API í• ë‹¹ëŸ‰ ì¶”ì ì„ ìœ„í•œ ì¹´ìš´í„° ì´ˆê¸°í™”
api_quota_usage = {
    "search.list": 0,
    "videos.list": 0,
    "channels.list": 0,
    "captions.list": 0,
    "total_cost": 0
}

# í• ë‹¹ëŸ‰ ë¹„ìš© (ë‹¨ìœ„)
QUOTA_COSTS = {
    "search.list": 100,
    "videos.list": 1,
    "channels.list": 1,
    "captions.list": 50
}

# -------------------- í•¨ìˆ˜ ì •ì˜ -------------------- #

def load_existing_video_ids():
    """ê¸°ì¡´ CSV íŒŒì¼ì—ì„œ ì´ë¯¸ ìˆ˜ì§‘í•œ ì˜ìƒ IDë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    existing_ids = set()
    try:
        if os.path.exists(CSV_FILE_NAME):
            df = pd.read_csv(CSV_FILE_NAME)
            if 'ì˜ìƒID' in df.columns:
                existing_ids = set(df['ì˜ìƒID'].tolist())
                print(f"ì´ë¯¸ ìˆ˜ì§‘ëœ {len(existing_ids)}ê°œ ì˜ìƒ IDë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"ê¸°ì¡´ ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    return existing_ids

def update_quota_usage(api_method):
    """API í˜¸ì¶œ í• ë‹¹ëŸ‰ ì‚¬ìš©ëŸ‰ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
    api_quota_usage[api_method] += 1
    api_quota_usage["total_cost"] += QUOTA_COSTS[api_method]
    
    if api_quota_usage["total_cost"] > 9000:
        print("âš ï¸ ê²½ê³ : API í• ë‹¹ëŸ‰ì˜ 90%ë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.")

def get_popular_game_videos(youtube, search_query, page_token=None):
    """ì¸ê¸° ìˆëŠ” ê²Œì„ ê´€ë ¨ ì˜ìƒì„ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    try:
        search_response = youtube.search().list(
            part="id,snippet",
            q=search_query,
            type="video",
            videoCategoryId="20",
            maxResults=VIDEOS_PER_REQUEST,
            regionCode="KR",
            relevanceLanguage="ko",
            publishedAfter=PUBLISHED_AFTER_DATE,
            publishedBefore=PUBLISHED_BEFORE_DATE,
            order="relevance",
            pageToken=page_token,
            fields="nextPageToken,items(id(videoId),snippet(channelId,channelTitle,publishedAt,title))"
        ).execute()
        
        update_quota_usage("search.list")
        return search_response
    except Exception as e:
        print(f"âŒ ì˜ìƒ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        time.sleep(5)
        return {"items": []}

def get_videos_details(youtube, video_ids):
    """ì—¬ëŸ¬ ë¹„ë””ì˜¤ IDì— ëŒ€í•œ ìƒì„¸ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    if not video_ids:
        return []
    
    try:
        video_response = youtube.videos().list(
            part="snippet,contentDetails,statistics",
            id=",".join(video_ids),
            fields="items(id,snippet(title,description,publishedAt,tags,thumbnails/high/url,channelId,channelTitle),contentDetails(duration,caption),statistics(viewCount,likeCount,commentCount))"
        ).execute()
        
        update_quota_usage("videos.list")
        return video_response.get("items", [])
    except Exception as e:
        print(f"âŒ ì˜ìƒ ìƒì„¸ ì •ë³´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        time.sleep(3)
        return []

def get_channel_stats(youtube, channel_ids):
    """ì±„ë„ ID ëª©ë¡ì— ëŒ€í•œ í†µê³„ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    if not channel_ids:
        return {}
    
    unique_channel_ids = list(set(channel_ids))
    channel_stats = {}
    
    for i in range(0, len(unique_channel_ids), 50):
        batch_channel_ids = unique_channel_ids[i:i+50]
        try:
            channel_response = youtube.channels().list(
                part="statistics",
                id=",".join(batch_channel_ids),
                fields="items(id,statistics(subscriberCount,viewCount,videoCount))"
            ).execute()
            
            update_quota_usage("channels.list")
            
            for item in channel_response.get("items", []):
                channel_stats[item["id"]] = item.get("statistics", {})
            
            time.sleep(0.5)
        except Exception as e:
            print(f"âŒ ì±„ë„ ì •ë³´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            time.sleep(3)
    
    return channel_stats

def get_video_transcript(video_id):
    """ì˜ìƒì˜ ìë§‰ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    transcript_text = ""
    transcript_type = "none"
    
    try:
        transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)
        try:
            transcript = transcript_list.find_manually_created_transcript(['ko'])
            transcript_data = transcript.fetch()
            transcript_text = TextFormatter().format_transcript(transcript_data)
            transcript_type = "manual_ko"
        except:
            try:
                transcript = transcript_list.find_generated_transcript(['ko'])
                transcript_data = transcript.fetch()
                transcript_text = TextFormatter().format_transcript(transcript_data)
                transcript_type = "auto_ko"
            except:
                pass
    except Exception as e:
        pass
    
    return transcript_text, transcript_type

def save_transcript_to_file(video_id, transcript_text):
    """ìë§‰ í…ìŠ¤íŠ¸ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    if not transcript_text:
        return ""
    
    try:
        transcript_file_path = os.path.join(TRANSCRIPTS_FOLDER_NAME, f"{video_id}.txt")
        with open(transcript_file_path, "w", encoding="utf-8") as f:
            f.write(transcript_text)
        return transcript_file_path
    except Exception as e:
        print(f"âŒ ìë§‰ íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return ""

def format_duration(seconds):
    """ì´ˆ ë‹¨ìœ„ ì‹œê°„ì„ 'ì‹œ:ë¶„:ì´ˆ' í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    hours = seconds // 3600
    minutes = (seconds % 3600) // 60
    seconds = seconds % 60
    return f"{int(hours)}:{int(minutes):02d}:{int(seconds):02d}" if hours else f"{int(minutes):02d}:{int(seconds):02d}"

def main():
    os.makedirs(TRANSCRIPTS_FOLDER_NAME, exist_ok=True)
    collected_video_ids = load_existing_video_ids()
    
    try:
        youtube = build('youtube', 'v3', developerKey=API_KEY)
    except Exception as e:
        print(f"âŒ YouTube API ì„œë¹„ìŠ¤ ê°ì²´ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return

    video_data = []
    successfully_collected = 0

    print(f"ğŸ® ê²Œì„ í‚¤ì›Œë“œë³„ ì¸ê¸° ì˜ìƒ ìˆ˜ì§‘ ì‹œì‘ (ëª©í‘œ: {MAX_TOTAL_VIDEOS_TO_COLLECT}ê°œ)")
    
    for search_query in SEARCH_KEYWORDS:
        if successfully_collected >= MAX_TOTAL_VIDEOS_TO_COLLECT:
            break
            
        print(f"\nğŸ” í˜„ì¬ í‚¤ì›Œë“œ: {search_query}")
        next_page_token = None
        
        while successfully_collected < MAX_TOTAL_VIDEOS_TO_COLLECT:
            if api_quota_usage["total_cost"] >= 9500:
                print("âš ï¸ API í• ë‹¹ëŸ‰ì´ ê±°ì˜ ì†Œì§„ë˜ì—ˆìŠµë‹ˆë‹¤.")
                break

            search_response = get_popular_game_videos(youtube, search_query, next_page_token)
            search_items = search_response.get("items", [])
            
            if not search_items:
                break

            video_ids = []
            for item in search_items:
                video_id = item["id"]["videoId"]
                if video_id not in collected_video_ids:
                    video_ids.append(video_id)
                    collected_video_ids.add(video_id)

            if not video_ids:
                next_page_token = search_response.get("nextPageToken")
                if not next_page_token:
                    break
                continue

            video_details = get_videos_details(youtube, video_ids)
            if not video_details:
                next_page_token = search_response.get("nextPageToken")
                if not next_page_token:
                    break
                continue

            channel_ids = [item.get("snippet", {}).get("channelId") for item in video_details if "snippet" in item]
            channel_stats = get_channel_stats(youtube, channel_ids)

            for video_item in video_details:
                if successfully_collected >= MAX_TOTAL_VIDEOS_TO_COLLECT:
                    break

                content_details = video_item.get("contentDetails", {})
                duration_iso = content_details.get("duration", "PT0S")
                duration_seconds = isodate.parse_duration(duration_iso).total_seconds()
                
                if not (MINIMUM_VIDEO_DURATION_SECONDS <= duration_seconds <= MAX_VIDEO_DURATION_SECONDS):
                    continue

                snippet = video_item.get("snippet", {})
                title = snippet.get('title', '')
                description = snippet.get('description', '')
                
                if not any(ord('ê°€') <= ord(c) <= ord('í£') for c in title + description):
                    continue

                video_id = video_item["id"]
                stats = video_item.get("statistics", {})
                channel_id = snippet.get("channelId", "")
                channel_stat_info = channel_stats.get(channel_id, {})

                transcript_text, transcript_type = get_video_transcript(video_id)
                save_transcript_to_file(video_id, transcript_text)

                video_data.append({
                    'ìˆ˜ì§‘ì¼ì': today_str,
                    'ì˜ìƒID': video_id,
                    'ì˜ìƒì œëª©': title,
                    'ì˜ìƒì„¤ëª…': description,
                    'ê²Œì‹œì¼': snippet.get('publishedAt', ''),
                    'íƒœê·¸': ', '.join(snippet.get('tags', [])),
                    'ì¸ë„¤ì¼URL': snippet.get('thumbnails', {}).get('high', {}).get('url', ''),
                    'ì¡°íšŒìˆ˜': int(stats.get('viewCount', 0)),
                    'ì¢‹ì•„ìš”ìˆ˜': int(stats.get('likeCount', 0)),
                    'ëŒ“ê¸€ìˆ˜': int(stats.get('commentCount', 0)),
                    'ì˜ìƒê¸¸ì´(ì´ˆ)': int(duration_seconds),
                    'ì˜ìƒê¸¸ì´(í‘œì‹œ)': format_duration(duration_seconds),
                    'ìë§‰ì—¬ë¶€': content_details.get('caption', 'false'),
                    'ì±„ë„ID': channel_id,
                    'ì±„ë„ëª…': snippet.get('channelTitle', ''),
                    'êµ¬ë…ììˆ˜': int(channel_stat_info.get('subscriberCount', 0)),
                    'ì±„ë„ì´ì¡°íšŒìˆ˜': int(channel_stat_info.get('viewCount', 0)),
                    'ì±„ë„ì—…ë¡œë“œì˜ìƒìˆ˜': int(channel_stat_info.get('videoCount', 0)),
                    'ìë§‰ìœ í˜•': transcript_type
                })

                successfully_collected += 1
                print(f"âœ… [{successfully_collected}/{MAX_TOTAL_VIDEOS_TO_COLLECT}] {title[:40]}...")

            next_page_token = search_response.get("nextPageToken")
            if not next_page_token:
                break
            time.sleep(random.uniform(1.0, 2.5))
            
            # âœ… í‚¤ì›Œë“œ ë‹¨ìœ„ ì‚¬ìš©ëŸ‰ ì¶œë ¥
            print(f"\nğŸ“Š '{search_query}' í‚¤ì›Œë“œ ìˆ˜ì§‘ í›„ ëˆ„ì  API ì‚¬ìš©ëŸ‰: {api_quota_usage['total_cost']}/10,000 ë‹¨ìœ„\n")
            time.sleep(random.uniform(1.5, 3.0))  # ë‹¤ìŒ í‚¤ì›Œë“œ ì „ ëŒ€ê¸°


    if video_data:
        df = pd.DataFrame(video_data)
        if os.path.exists(CSV_FILE_NAME):
            existing_df = pd.read_csv(CSV_FILE_NAME)
            combined_df = pd.concat([existing_df, df], ignore_index=True)
            combined_df.drop_duplicates(subset=['ì˜ìƒID'], keep='first', inplace=True)
            combined_df.to_csv(CSV_FILE_NAME, index=False, encoding='utf-8-sig')
            print(f"\nâœ… ì´ {len(combined_df)}ê°œ ì˜ìƒ ë°ì´í„° ì €ì¥ ì™„ë£Œ (ê¸°ì¡´ {len(existing_df)}ê°œ + ìƒˆë¡œìš´ {len(df)}ê°œ)")
        else:
            df.to_csv(CSV_FILE_NAME, index=False, encoding='utf-8-sig')
            print(f"\nâœ… ì´ {len(df)}ê°œ ì˜ìƒ ë°ì´í„° ì €ì¥ ì™„ë£Œ")
    else:
        print("\nâŒ ìˆ˜ì§‘ëœ ì˜ìƒì´ ì—†ìŠµë‹ˆë‹¤.")

    print("\nğŸ“Š API í• ë‹¹ëŸ‰ ì‚¬ìš© í˜„í™©:")
    for method, count in api_quota_usage.items():
        if method != "total_cost":
            print(f"  - {method}: {count}íšŒ í˜¸ì¶œ ({count * QUOTA_COSTS[method]} ë‹¨ìœ„)")
    print(f"  - ì´ ì‚¬ìš©ëŸ‰: {api_quota_usage['total_cost']}/10,000 ë‹¨ìœ„")

if __name__ == "__main__":
    main()
